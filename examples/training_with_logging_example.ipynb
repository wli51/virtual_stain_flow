{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e76a09d1",
   "metadata": {},
   "source": [
    "# Training U-Net with logging Example Notebook\n",
    "\n",
    "This notebook demonstrates how to train `virtual_stain_flow.models` module,\n",
    "demoing with trainer and logging. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7931cd06",
   "metadata": {},
   "source": [
    "## Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db534211",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from torchmetrics.image import MultiScaleStructuralSimilarityIndexMeasure\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "from virtual_stain_flow.trainers.logging_trainer import SingleGeneratorTrainer\n",
    "from virtual_stain_flow.vsf_logging.MlflowLogger import MlflowLogger\n",
    "from virtual_stain_flow.vsf_logging.callbacks.PlotCallback import PlotPredictionCallback\n",
    "from virtual_stain_flow.models.unet import UNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878fbb1f",
   "metadata": {},
   "source": [
    "## Additional utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e73b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Simple dataset for demo purposes.\n",
    "    Loads images from disk, crops the center, and performs hard-coded normalization.\n",
    "    \"\"\"\n",
    "    def __init__(self, index_df, data_dir):\n",
    "        self.index_df = index_df\n",
    "        self.data_dir = data_dir\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.index_df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.index_df.iloc[idx]\n",
    "        \n",
    "        # Load images\n",
    "        brightfield_path = self.data_dir / 'data' / row['OrigBrightfield']\n",
    "        dna_path = self.data_dir / 'data' / row['OrigDNA']\n",
    "        \n",
    "        brightfield_img = Image.open(brightfield_path)\n",
    "        brightfield_img.convert('I;16')\n",
    "        dna_img = Image.open(dna_path)\n",
    "        dna_img.convert('I;16')\n",
    "        brightfield_img = np.asarray(brightfield_img, dtype=np.float32)\n",
    "        dna_img = np.asarray(dna_img, dtype=np.float32)\n",
    "\n",
    "        # Get image dimensions\n",
    "        width, height = brightfield_img.shape\n",
    "        \n",
    "        # Calculate crop coordinates for center 256x256\n",
    "        left = (width - 256) // 2\n",
    "        top = (height - 256) // 2\n",
    "        right = left + 256\n",
    "        bottom = top + 256\n",
    "        \n",
    "        # Crop center 256x256\n",
    "        brightfield_crop = brightfield_img[top:bottom,left:right]\n",
    "        dna_crop = dna_img[top:bottom,left:right]\n",
    "\n",
    "        # Normalize by 16bit max value\n",
    "        factor = 2 ** 16 - 1\n",
    "        brightfield_crop = brightfield_crop / factor\n",
    "        dna_crop = dna_crop / factor\n",
    "\n",
    "        # Convert to tensor\n",
    "        brightfield_tensor = torch.from_numpy(brightfield_crop).unsqueeze(0)  # Add channel dimension\n",
    "        dna_tensor = torch.from_numpy(dna_crop).unsqueeze(0)  # Add channel dimension\n",
    "\n",
    "        return brightfield_tensor, dna_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c37718bd",
   "metadata": {},
   "source": [
    "## Retrieve Demo Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af0393f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pathlib.Path(\n",
    "    \"/mnt/data_nvme1/data/alsf_portable/12000_train_set\"\n",
    ").resolve(strict=True)\n",
    "\n",
    "if not data_dir.exists():\n",
    "    raise FileNotFoundError(f\"Data directory {data_dir} does not exist.\")\n",
    "\n",
    "dataset_index_json = data_dir / \"dataset.json\"\n",
    "\n",
    "# dataset_index_json = list(data_dir.rglob(\"dataset.json\"))\n",
    "if not dataset_index_json:\n",
    "    raise FileNotFoundError(f\"No dataset.json found in {data_dir}.\")\n",
    "# dataset_index_json = dataset_index_json[0]\n",
    "\n",
    "with open(dataset_index_json, \"r\") as f:\n",
    "    dataset_index = json.load(f)\n",
    "\n",
    "index = pd.DataFrame(dataset_index['file_index']['records'])\n",
    "print(f\"Total number of samples: {len(index)}\")\n",
    "index = index.sample(n=40, random_state=42).reset_index(drop=True)\n",
    "print(f\"Subset number of samples: {len(index)}\")\n",
    "print(index.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0bd67",
   "metadata": {},
   "source": [
    "## Peek several patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53533b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset instance\n",
    "dataset = SimpleDataset(index, data_dir)\n",
    "print(f\"Dataset created with {len(dataset)} samples\")\n",
    "\n",
    "# Plot the first 5 samples from the dataset\n",
    "fig, axes = plt.subplots(5, 2, figsize=(8, 16))\n",
    "\n",
    "for i in range(5):\n",
    "    brightfield, dna = dataset[i]\n",
    "    brightfield = brightfield.numpy().squeeze()\n",
    "    dna = dna.numpy().squeeze()\n",
    "\n",
    "    # Plot brightfield image\n",
    "    axes[i, 0].imshow(brightfield.squeeze(), cmap='gray')\n",
    "    axes[i, 0].set_title(f'Sample {i} - Brightfield')\n",
    "    axes[i, 0].axis('off')\n",
    "    \n",
    "    # Plot DNA image\n",
    "    axes[i, 1].imshow(dna.squeeze(), cmap='gray')\n",
    "    axes[i, 1].set_title(f'Sample {i} - DNA')\n",
    "    axes[i, 1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc0c066f",
   "metadata": {},
   "source": [
    "## Configure and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2f78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "epochs = 100\n",
    "learning_rate = 0.001 # larger learning rate for demo purposes\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Batch with DataLoader\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Model & Optimizer\n",
    "fully_conv_unet = UNet(\n",
    "    in_channels=1,\n",
    "    out_channels=1,\n",
    "    depth=4,\n",
    "    encoder_down_block='conv',\n",
    "    decoder_up_block='convt',\n",
    "    act_type='sigmoid'\n",
    ")\n",
    "optimizer = torch.optim.Adam(fully_conv_unet.parameters(), lr=learning_rate)\n",
    "\n",
    "# Plotting callback to visualize predictions during training\n",
    "plot_callback = PlotPredictionCallback(\n",
    "    name=\"plot_callback_with_train_data\",\n",
    "    dataset=dataset,\n",
    "    indices=[0,1,2,3,4], # first 5 samples\n",
    "    plot_metrics=[torch.nn.L1Loss()],\n",
    "    every_n_epochs=5,\n",
    "    show_plot=False\n",
    ")\n",
    "\n",
    "# MLflow Logger\n",
    "logger = MlflowLogger(\n",
    "    name=\"logger\",\n",
    "    experiment_name=\"vsf_examples\",\n",
    "    tracking_uri=\"http://127.0.0.1:5000\",\n",
    "    run_name=\"experiment_training_with_plots\",\n",
    "    description=\"Training a UNet model on a simple dataset for demo purposes\",\n",
    "    callbacks=[plot_callback],\n",
    "    save_model_at_train_end=True,\n",
    "    save_model_every_n_epochs=1,\n",
    "    save_best_model=True\n",
    ")\n",
    "\n",
    "# Initialize Trainer and start training\n",
    "trainer = SingleGeneratorTrainer(\n",
    "    model=fully_conv_unet,\n",
    "    optimizer=optimizer,\n",
    "    losses=[\n",
    "        torch.nn.L1Loss(),\n",
    "        MultiScaleStructuralSimilarityIndexMeasure(\n",
    "            data_range=1.0,\n",
    "            kernel_size=11,\n",
    "            sigma=1.5,\n",
    "        )\n",
    "    ],\n",
    "    loss_weights=[1.0, -1.0], # minimize L1 distance and maximize MS-SSIM\n",
    "    device=device,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=None,\n",
    "    test_loader=None,\n",
    ")\n",
    "\n",
    "trainer.train(logger=logger, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "499c2703",
   "metadata": {},
   "source": [
    "## Visualize training outcome through MLflow client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7242b8",
   "metadata": {},
   "source": [
    "### Display the last logged prediction plot artifact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac03aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLflow client\n",
    "client = MlflowClient(tracking_uri=\"http://127.0.0.1:5000\")\n",
    "\n",
    "# Get the experiment by name\n",
    "experiment = client.get_experiment_by_name(\"vsf_examples\")\n",
    "\n",
    "if experiment is None:\n",
    "    print(\"Experiment 'vsf_examples' not found\")\n",
    "else:\n",
    "    # Search for runs with the specific run name\n",
    "    runs = client.search_runs(\n",
    "        experiment_ids=[experiment.experiment_id],\n",
    "        filter_string=\"tags.mlflow.runName = 'experiment_training_with_plots'\"\n",
    "    )\n",
    "    \n",
    "    if len(runs) == 0:\n",
    "        print(\"No runs found with name 'experiment_training_with_plots'\")\n",
    "    else:\n",
    "        # Get the most recent run (first in list)\n",
    "        run = runs[0]\n",
    "        print(f\"Run ID: {run.info.run_id}\")\n",
    "        print(f\"Run Name: {run.data.tags.get('mlflow.runName')}\")\n",
    "\n",
    "        plot_artifacts = client.list_artifacts(run.info.run_id, path='plots/epoch/plot_predictions/')\n",
    "\n",
    "        # Filter for PNG files and sort by path (which includes epoch number)\n",
    "        png_files = [artifact for artifact in plot_artifacts if artifact.path.endswith('.png')]\n",
    "\n",
    "        # Get full paths and sort by epoch number\n",
    "        png_files_sorted = sorted(png_files, key=lambda x: int(x.path.split('_')[-1].split('.')[0]))\n",
    "        most_recent_png = png_files_sorted[-1]\n",
    "\n",
    "        print(f\"Last epoch plot: {most_recent_png.path}\")\n",
    "\n",
    "        # Download and display the image\n",
    "        local_path = client.download_artifacts(run.info.run_id, most_recent_png.path)\n",
    "        img = Image.open(local_path)\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Predictions - {most_recent_png.path.split('/')[-1]}\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7adead3c",
   "metadata": {},
   "source": [
    "### Also visualize metrics from tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15642c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_keys = list(run.data.metrics.keys()) or []\n",
    "\n",
    "for metric_name in metric_keys:\n",
    "    # 2. Get full history for each metric (all steps)\n",
    "    history = client.get_metric_history(run.info.run_id, metric_name)\n",
    "    if not history:\n",
    "        continue\n",
    "\n",
    "    steps = [m.step for m in history]\n",
    "    values = [m.value for m in history]\n",
    "\n",
    "    # 3. Plot each metric vs step\n",
    "    plt.figure()\n",
    "    plt.plot(steps, values, marker=\"o\")\n",
    "    plt.title(f\"{metric_name} (run {run.info.run_id})\")\n",
    "    plt.xlabel(\"step\")\n",
    "    plt.ylabel(metric_name)\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "virtual_stain_flow (3.12.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
